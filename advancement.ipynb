{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"A Latent Variable Model for Multivariate Mixed Time-Series Data\"\n",
        "subtitle: With Applications to Sports Production Curve Modeling\n",
        "author: Abhijit Brahme\n",
        "format: \n",
        "    revealjs: \n",
        "        incremental: true\n",
        "        smaller: true\n",
        "        scrollable: true\n",
        "        html-math-method: mathjax\n",
        "        theme: solarized\n",
        "server: shiny\n",
        "date: 2024-09-20\n",
        "bibliography: advancement.bib\n",
        "---"
      ],
      "id": "5290a1b7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "context": "setup"
      },
      "source": [
        "#| include: false\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import pickle\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import SparsePCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from shiny.express import input, render, ui\n",
        "from shinywidgets import render_plotly\n",
        "import numpyro\n",
        "from tensorly.decomposition import tucker\n",
        "numpyro.set_platform(\"cpu\")"
      ],
      "id": "4521234a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "context": "setup",
        "cache": true
      },
      "source": [
        "#| include: false\n",
        "from data.data_utils import create_fda_data\n",
        "data = pd.read_csv(\"data/player_data.csv\").query(\" age <= 38 \")\n",
        "names = data.groupby(\"id\")[\"name\"].first().values\n",
        "names_df = pd.DataFrame(names, columns = [\"Name\"])\n",
        "names_df[\"Player\"] = range(len(names))\n",
        "metric_output = [\"binomial\", \"exponential\"] + ([\"gaussian\"] * 2) + ([\"poisson\"] * 9) + ([\"binomial\"] * 3)\n",
        "metrics = [\"retirement\", \"minutes\", \"obpm\",\"dbpm\",\"blk\",\"stl\",\"ast\",\"dreb\",\"oreb\",\"tov\",\"fta\",\"fg2a\",\"fg3a\",\"ftm\",\"fg2m\",\"fg3m\"]\n",
        "metric_df = pd.DataFrame(metrics, columns=[\"Statistic\"])\n",
        "metric_df[\"Metric\"] = range(len(metrics))\n",
        "age_df = pd.DataFrame(range(18,39), columns = [\"Age\"])\n",
        "age_df[\"Time\"] = age_df[\"Age\"] - 18\n",
        "exposure_list = ([\"simple_exposure\"] * 2) + ([\"minutes\"] * 11) + [\"fta\",\"fg2a\",\"fg3a\"]\n",
        "data[\"retirement\"] = 1\n",
        "data[\"log_min\"] = np.log(data[\"minutes\"])\n",
        "data[\"simple_exposure\"] = 1 \n",
        "_ , outputs, _ = create_fda_data(data, basis_dims=3, metric_output=metric_output, \n",
        "                                     metrics = metrics\n",
        ", exposure_list =  exposure_list)\n",
        "observations = np.stack([output[\"output_data\"] for output in outputs], axis = 1)\n",
        "exposures = np.stack([output[\"exposure_data\"] for output in outputs], axis = 0)\n",
        "\n",
        "agg_dict = {\"obpm\":\"mean\", \"dbpm\":\"mean\", \"bpm\":\"mean\", \n",
        "            \"position_group\": \"max\",\n",
        "        \"minutes\":\"sum\", \"dreb\": \"sum\", \"fta\":\"sum\", \"ftm\":\"sum\", \"oreb\":\"sum\",\n",
        "        \"ast\":\"sum\", \"tov\":\"sum\", \"fg2m\":\"sum\", \"fg3m\":\"sum\", \"fg3a\":\"sum\", \"fg2a\":\"sum\", \"blk\":\"sum\", \"stl\":\"sum\"}\n",
        "data[\"total_minutes\"] = data[\"median_minutes_per_game\"] * data[\"games\"] \n",
        "agged_data = data.groupby(\"id\").agg(agg_dict).reset_index()\n",
        "agged_data[\"ft_pct\"] = agged_data[\"ftm\"] / agged_data[\"fta\"]\n",
        "agged_data[\"fg2_pct\"] = agged_data[\"fg2m\"] / agged_data[\"fg2a\"]\n",
        "agged_data[\"fg3_pct\"] = agged_data[\"fg3m\"] / agged_data[\"fg3a\"]\n",
        "agged_data[\"dreb_rate\"] = 36.0 * agged_data[\"dreb\"] / agged_data[\"minutes\"]\n",
        "agged_data[\"oreb_rate\"] = 36.0 * agged_data[\"oreb\"] / agged_data[\"minutes\"]\n",
        "agged_data[\"ast_rate\"] = 36.0 * agged_data[\"ast\"] / agged_data[\"minutes\"]\n",
        "agged_data[\"tov_rate\"] = 36.0 * agged_data[\"tov\"] / agged_data[\"minutes\"]\n",
        "agged_data[\"blk_rate\"] = 36.0 * agged_data[\"blk\"] / agged_data[\"minutes\"]\n",
        "agged_data[\"stl_rate\"] = 36.0 * agged_data[\"stl\"] / agged_data[\"minutes\"]\n",
        "agged_data[\"ft_rate\"] = 36.0 * agged_data[\"fta\"] / agged_data[\"minutes\"]\n",
        "agged_data[\"fg2_rate\"] = 36.0 * agged_data[\"fg2a\"] / agged_data[\"minutes\"]\n",
        "agged_data[\"fg3_rate\"] = 36.0 * agged_data[\"fg3a\"] / agged_data[\"minutes\"]\n",
        "agged_data.fillna(0, inplace=True)"
      ],
      "id": "c4dac246",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "context": "setup",
        "cache": true
      },
      "source": [
        "#| include: false\n",
        "\n",
        "N, K, T = observations.shape\n",
        "\n",
        "edges = []\n",
        "nodes = []\n",
        "vals = []\n",
        "missing = []\n",
        "for player_index in range(N):\n",
        "    for metric_index in range(K):\n",
        "        for time_index in range(T):\n",
        "            cur_node = (player_index, metric_index, time_index)\n",
        "            nodes.append(cur_node)\n",
        "            missing.append(np.isfinite(exposures[metric_index, player_index, time_index]))\n",
        "            vals.append(observations[player_index, metric_index, time_index])\n",
        "\n",
        "            if player_index != N - 1:\n",
        "                edges.append((cur_node, (player_index + 1, metric_index ,time_index)))\n",
        "\n",
        "            if player_index != 0:\n",
        "                edges.append((cur_node, (player_index - 1, metric_index, time_index )))\n",
        "            \n",
        "            if metric_index != K - 1:\n",
        "                edges.append((cur_node, (player_index, metric_index + 1, time_index)))\n",
        "\n",
        "            if metric_index != 0:\n",
        "                edges.append((cur_node, (player_index, metric_index - 1, time_index)))\n",
        "\n",
        "            if time_index != T - 1:\n",
        "                edges.append((cur_node, (player_index, metric_index, time_index + 1)))\n",
        "\n",
        "            if time_index != 0:\n",
        "                edges.append((cur_node, (player_index, metric_index, time_index - 1)))\n",
        "\n",
        "nodes_df = pd.DataFrame(np.array([list(x) for x in zip(*nodes)]).T, columns=[\"Player\", \"Metric\", \"Time\"])\n",
        "nodes_df[\"Value\"] = vals\n",
        "nodes_df[\"Missing\"] = missing\n",
        "\n",
        "final_data_plot_df = pd.merge(nodes_df, names_df).merge(metric_df).merge(age_df)"
      ],
      "id": "8032063e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with open(\"model_output/exponential_cp_test.pkl\", \"rb\") as f:\n",
        "    results = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "X = results[\"U_auto_loc\"]\n",
        "\n",
        "U, _, _ = np.linalg.svd(X, full_matrices=False)\n",
        "L       = np.linalg.cholesky(np.cov(U.T) + 1e-6 * np.eye(7)).T\n",
        "aligned_X  = np.linalg.solve(L, U.T).T\n",
        "aligned_X /= np.std(X, axis=0)\n",
        "\n",
        "X_tsne = TSNE(n_components=3).fit_transform(aligned_X)\n",
        "\n",
        "\n",
        "scatter_df = pd.concat([pd.DataFrame(X_tsne, columns=[f\"dim{i+1}\" for i in range(3)]), agged_data], axis = 1)\n",
        "scatter_df[\"player_name\"] = names"
      ],
      "id": "ea8d0691",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import jax.numpy as jnp\n",
        "with open(\"model_output/fixed_nba_tvrflvm_test.pkl\", \"rb\") as f:\n",
        "    results_tvrflvm = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "W = results_tvrflvm[\"W\"]\n",
        "\n",
        "with open(\"model_output/tensor_decomposition.pkl\", \"rb\") as f:\n",
        "    results_tensor = pickle.load(f)\n",
        "f.close()\n",
        "core, factors, mu = results_tensor[\"core\"], results_tensor[\"factors\"], results_tensor[\"mu\"]\n",
        "full_core = np.einsum(\"nkl,lj -> nkj\", np.einsum(\"nc,ckl -> nkl\", factors[0], core), factors[2])\n",
        "pos_indices = data.groupby(\"id\")[\"position_group\"].max().reset_index()\n",
        "positions = [\"G\", \"F\", \"C\"]\n",
        "tensor_decomps = {}\n",
        "tensor_decomps[\"global\"] = {\"core\": core, \"factors\": factors, \"full_core\": full_core}\n",
        "for pos in positions:\n",
        "    core, factors = results_tensor[f\"core_{pos}\"], results_tensor[f\"factors_{pos}\"]\n",
        "    full_pos_core = np.einsum(\"nkl,lj -> nkj\", np.einsum(\"nc,ckl -> nkl\", factors[0], core), factors[2])\n",
        "    tensor_decomps[pos] = {\"core\": core, \"factors\": factors, \"full_core\": full_pos_core}"
      ],
      "id": "3bccf645",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Research Interests \n",
        "\n",
        "::: {layout-ncol=2}\n",
        "![](images/noun-deep-learning-1705425.png)\n",
        "\n",
        "![](images/noun-scatter-graph-4768711.png)\n",
        ":::\n",
        "\n",
        "\n",
        "::: {.notes}\n",
        "Generally work at the intersection of more modern advances in machine learning and statistics. Trying to find connections between these advances, and \n",
        "even instances where one field could help the other. \n",
        ":::\n",
        "\n",
        "## Current Work\n",
        "\n",
        "1. Metric / Manifold Estimation from Trajectory Valued Data \n",
        "    - Prof. Nina Mialone (Dept. of Electrical Engineering)\n",
        "2. Approximate HMC Methods\n",
        "    - Prof. Andrew Holbrook (UCLA, Dept of Public Health)\n",
        "3. **A Latent Variable Model for Modeling Multi-Dimensional Mixed Time-Series Data**\n",
        "    - **Prof. Alex Franks (Dept. of Statistics)**\n",
        "\n",
        "## Motivation \n",
        "\n",
        "::: {.nonincremental}\n",
        "1. Successfully forecast player performance, especially young players\n",
        "2. Gain insight into how different observable metrics may peak at varying timepoints for certain groups of players\n",
        "3. Quantify a level of uncertainty regarding players' future performance\n",
        "4. Jointly model all of the above \n",
        "::: \n"
      ],
      "id": "e5779f36"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "### combine with next slide maybe\n",
        "from visualization.visualization import plot_career_trajectory_observations\n",
        "ui.input_select(id=\"player_traj\", label = \"Select a player\", choices = {index : name for index, name in enumerate(names)})\n",
        "@render_plotly\n",
        "def plot_metric_arc():\n",
        "    return plot_career_trajectory_observations(int(input.player_traj()), metrics, metric_output, observations, exposures )"
      ],
      "id": "b1cf7c77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.notes}\n",
        "The data are these time varying curves of specific observable and recorded metrics in basketball. Each one of these metrics is sort of an emission from an underlying latent\n",
        "state or ability. For example, we can look at LeBron James, who has had a long career and is in fact still playing. Some of these metrics are more \"skill\" based in that they aren't affected by physical aging. For example, the ability to make free-throws is not really an athletic feat. Whereas, things like getting rebounds or scoring, are affected by age. So for example, there is some variation amongst metrics themselves in where they peak, as well as variation within individuals, usually by position. \n",
        "\n",
        "So, if we look at Stephen Curry by contrast, he is a smaller guard. He is shorter, and not as athletic as LeBron, and so drop-off in his athleticism will appear more sharp as it is not mitigated by physical strength or height. The model, nor the data, doesn't encode any of this explicitly, but the goal is to learn such variations through a latent embedding of players. We want to learn how to place players in some latent space where their position in the latent space tells us how they will age. \n",
        "\n",
        "More so, we want to establish some measure of uncertainty about where players are placed. This is typically more important for rookies (new players) who just entered the league and we are sort of looking to model their future performance. Sort of an important task for team managers is to project performance and putting a level of uncertainty to an estimate is helpful in decision making. \n",
        "\n",
        "Additionally, we want to be able to learn something about aging in general. How do different types of players age? How does that affect where they peak in certain metrics?\n",
        "So to do all of this, we want to model everything jointly. \n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Data Overview\n",
        "\n",
        "::: {.nonincremental}\n",
        "1. $\\approx$ 3,000 NBA players from years 1997 - 2021, from the ages of 18 - 39\n",
        "2. Longitudinal mixed tensor valued data $\\mathcal{Y}$ of size $N$ by $T$ by $K$\n",
        "where $N$ is the number of players, $T$ is the number of years in a player's career, and $K$ are the number of production metric curves with $\\mathcal{Y}_{ntk}$ is missing if player $n$ is not observed for metric $k$ at age $t$. \n",
        "    - Non-missing entries are observations from exponential families (i.e Binomial, Gaussian, Exponential, Poisson, etc.)\n",
        "3. $\\Omega$ is binary tensor of same size as $\\mathcal{Y}$ indicating missingness. \n",
        ":::\n"
      ],
      "id": "ab70dac5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#### change time to actual age \n",
        "from visualization.visualization import plot_data_tensor\n",
        "@render_plotly\n",
        "def plot_data():\n",
        "    return plot_data_tensor(final_data_plot_df)"
      ],
      "id": "14749332",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Relevant Work\n",
        "\n",
        "1. Bayesian Hierarchical Framework\n",
        "    - Hierarchical aging model applied to hockey, golf, and baseball [@berry1999bridging]\n",
        "    - Gaussian Process regressions by different basketball positions [@page2013effect]\n",
        "    - Parametric curves analyzing pre and post peak-performance [@vaci2019large]\n",
        "\n",
        "2. Functional Data Analysis\n",
        "    - Functional principal components clustering [@wakim2014functional]\n",
        "    - Nearest Neighbor algorithm to characterize similarity between players [@natesilver538_2015]\n",
        "    - Production curves as a convex combination of curves from the same set of archetype [@vinue2015archetypoids]\n",
        "\n",
        "\n",
        "::: {.notes}\n",
        "So there's already been some kind of work in this area already. Some of this work is using specifically player positions to develop projections. But, these positions are kind of rigid in explaining the intricacies of basketball and similarities between players. We really want a continuous measure. \n",
        "\n",
        "The paper that is most similar is this functional principal components clustering, but that works on the space of 1 metric / measure. and Nate silver's method. Again, they are not jointly learning these things. \n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Modeling Setup\n",
        "Space of players live in low dimensional latent space $X \\in \\mathbb{R} ^{N \\times D}$\n",
        "\n",
        "For a given time $t$, and metric $k$, $f^k_t \\sim \\mathcal{GP}(0, K_X)$ is a vector of size $N$, with $K_X$ capturing correlation across the $N$ dimension [@gundersen2020latent]\n",
        "\n",
        ":::{layout-ncol=2}\n",
        "![](images/pgm.svg){width=90%}\n",
        "\n",
        "\n",
        "\\begin{pmatrix}\n",
        "  \\vert & \\vert & \\cdots & \\vert & \\cdots & \\vert  \\\\\n",
        "  \\vert & \\vert & \\cdots & \\vert & \\cdots & \\vert  \\\\\n",
        "  f^k_1 & f^k_2 & \\cdots & f^k_i & \\cdots & f^k_T  \\\\\n",
        "  \\vert  & \\vert & \\cdots & \\vert & \\cdots & \\vert \\\\\n",
        "  \\vert  & \\vert & \\cdots  & \\vert & \\cdots  & \\vert \n",
        "\\end{pmatrix}\n",
        ":::\n",
        "\n",
        "## Random Fourier Features \n",
        "\n",
        "Attempt to approximate the inner product $k(x, y) = \\langle \\phi(x), \\phi(y) \\rangle$\n",
        "with a randomized map $z: \\mathbb{R}^D \\rightarrow \\mathbb{R}^R$. Computational savings arise if $R << N$. \n",
        "\n",
        "**Approximation of Gaussian Process can be turned into a linear operation, $f^k_{t}(X) \\approx Z(X)^T\\beta^k_{t}$ which is computationally beneficial if $N$ large.**\n",
        "\n",
        "In our case, we let $k(x,y) = k(x - y) = exp(\\frac{-||x - y||^2}{2})$ be the standard radial basis kernel. \n",
        "\n",
        "It can be shown that to produce the radial basis kernel, $\\omega \\sim \\mathcal{N}_D(0, I_d)$. \n",
        "\n",
        "Thus the map is composed of $z_{\\omega_r} = [cos(\\omega_r ^T x), sin(\\omega_r ^ T x)]^T$. \n",
        "\n",
        "$Z(X) = \\frac{1}{\\sqrt{R}}[z_{\\omega_1}, z_{\\omega_2}, \\dots, z_{\\omega_R}]^T$\n",
        "\n",
        "## Adjusted Modeling Setup\n",
        "\n",
        "![](images/pmg_corr.svg){fig-align=\"center\"}\n",
        "\n",
        "## Distributional Assumptions\n",
        "\n",
        "We include the following metrics and distribution families:\n",
        "\n",
        ":::{layout-ncol=2}\n",
        "\n",
        "::: {.nonincremental}\n",
        "1. Poisson\n",
        "    - $\\mathcal{R} =$ {FG2A, FG3A, FTA, BLK, OREB, DREB, TOV, AST, STL}\n",
        "2. Gaussian\n",
        "    - $\\mathcal{G} =$ {DBPM, OBPM}\n",
        "3. Binomial\n",
        "    - $\\mathcal{B} =$ {FG2M, FG3M, FTM}\n",
        "     - $\\mathcal{A} =$ {FG2A, FG3A, FTA}\n",
        "4. Exponential\n",
        "    - $\\mathcal{M} =$ {Minutes}\n",
        "5. Bernoulli\n",
        "    - $\\mathcal{K} =$ {Retirement}\n",
        ":::\n",
        "\n",
        "\\begin{align}\n",
        "Y^k_{t}  &\\sim\n",
        "\\begin{cases}\n",
        "Pois(Y^m_t e^{f^k_{t}}) \\text{if } k \\in \\mathcal{R} \\text{ , } \\forall m \\in \\mathcal{M} \\\\\n",
        "Bin(Y^j_{t}, logit^{-1}(f^k_{t})) \\text{ if } k \\in \\mathcal{B} \\text{ , }  j \\in \\mathcal{A} \\\\\n",
        "\\mathcal{N}(f^k_{t}, \\frac{\\sigma^2_k}{Y^m_t})  \\text{ if } k \\in \\mathcal{G} \\text{ , } \\forall m \\in \\mathcal{M}\\\\\n",
        "Bern(logit^{-1}(f^k_t)) \\text{ if } k \\in \\mathcal{K}\\\\\n",
        "Exp(e^{f^k_t}) \\text{ if  } k \\in \\mathcal{M}\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        ":::\n",
        "\n",
        "::: {.notes}\n",
        "One thing to take note of here is that we are handling exposure parameters properly here. So, for example the total rebounds is scaled by the number of minutes played that year. Similarly for the metrics like 3 point percentage, we include the number of attempts (which coincidentally is another metric). The gaussian metric's variance is scaled by the minutes played as well. So in this way we handle varying levels of playing time (something that other papers don't really look at.) \n",
        ":::\n",
        "\n",
        "\n",
        "## Inferential Challenges \n",
        "\n",
        "1. MCMC Convergence (multi-modal posterior)\n",
        "2. Identifiability (rotational / scale invariance of model)\n",
        "\n",
        "## Methods \n",
        "\n",
        "In order to address identifiability issues and MCMC convergence, we propose the following scheme to estimate the latent space $X$ and functional coefficients $\\beta_{rtk}$. \n",
        "\n",
        "1. Initialize $X$ \n",
        "    - Probabilistic Tensor Decomposition\n",
        "        - Helps better encapsulate similarities between players, so good to initialize with good similarity \n",
        "2. Using the fixed $X$ from above, conduct inference on $\\beta_{rtk}, \\sigma_k, \\omega_r$ \n",
        "\n",
        "## Probabilistic Tensor Decomposition \n",
        "\n",
        ":::{layout-ncol=2}\n",
        "![](images/cp_decomp.jpg){width=90%}\n",
        "\n",
        ":::\n",
        "\n",
        "This model seeks to factorize the  $N \\times T \\times K$ linear scale tensor $A$ using CP Decomposition. Since we have various outputs that are not normally distributed, this becomes a form of exponential family CP Decomposition.\n",
        "\n",
        "\n",
        "## Probabilistic Tensor Decomposition (contd.) \n",
        "\n",
        "Let $\\tilde{A}_{pit} = g_{pit}^{-1}(A_{pit})$, where $g_{pit}$ is the appropriate link function transforming the linear scale parameter into the appropriate exponential family parameterization. Consequently, $X, V, W, \\mu$ are estimated by maximizing the following loss function using gradient descent. \n",
        "\n",
        "$\\max_{X, V, W, \\mu} \\sum_{p, i, t} log( F_{pit}( Y_{pit} | \\tilde{A}_{pit} )) \\cdot \\Omega_{pit}$\n",
        "\n",
        "where $F_{pit}$ is the appropriate distribution density function associated with entry $Y_{pit}$. \n",
        "\n",
        "This offers the following benefits:\n",
        "\n",
        "1. Latent space $X$ is created while accounting for sampling variability \n",
        "2. Latent space is created while also accounting for correlations across each mode of the tensor, which is representative of the final model. \n",
        "\n",
        "\n",
        "## Latent Space \n"
      ],
      "id": "65178967"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from visualization.visualization import plot_scatter\n",
        "ui.input_select(id=\"player\", label = \"Select a player\", choices = {index : name for index, name in enumerate(names)})\n",
        "\n",
        "@render_plotly\n",
        "def plot_latent_space():\n",
        "    return plot_scatter(scatter_df,  \"Latent Embedding\", int(input.player()) )"
      ],
      "id": "d1678c65",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Projection Results \n"
      ],
      "id": "63c3e727"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from visualization.visualization import plot_posterior_predictive_career_trajectory\n",
        "import jax.numpy as jnp\n",
        "ui.input_select(id=\"player_posterior\", label = \"Select a player\", choices = {index : name for index, name in enumerate(names)})\n",
        "\n",
        "\n",
        "@render_plotly\n",
        "def plot_posterior_predictive():\n",
        "    player_index = int(input.player_posterior())\n",
        "    wTx = jnp.einsum(\"r,ijmr -> ijm\", aligned_X[player_index, :], W)\n",
        "    phi = jnp.concatenate([jnp.cos(wTx), jnp.sin(wTx)], -1) * (1/ jnp.sqrt(100))\n",
        "    mu = jnp.einsum(\"ijk,ijmkt -> ijmt\", phi, results_tvrflvm[\"beta\"])\n",
        "    \n",
        "    return plot_posterior_predictive_career_trajectory(\n",
        "                                                            player_index=player_index,\n",
        "                                                            metrics=metrics, \n",
        "                                                            metric_outputs=metric_output,\n",
        "                                                            posterior_mean_samples=mu,\n",
        "                                                            observations=jnp.array(observations), \n",
        "                                                            exposures = jnp.array(exposures),\n",
        "                                                            posterior_variance_samples=jnp.stack([results_tvrflvm[\"sigma_obpm\"], results_tvrflvm[\"sigma_dbpm\"]], axis = 0))"
      ],
      "id": "008772dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functional Basis \n",
        "\n",
        "![](images/tuckerdecomp.png)\n",
        "\n",
        "Let $\\mathcal{X}$ be the tensor of normalized gaussian-scale linear predictors of dimension $N \\times T \\times K$. \n",
        "Then, $\\mathcal{X}$ is decomposed into a \"core\" tensor $\\mathcal{Y}$ of dimension $N \\times M \\times K$ and \"factor\" matrices $A \\in \\mathbb{R}^{N \\times N}$, $B \\in \\mathbb{R}^{T \\times M}$, $C \\in \\mathbb{R}^{K \\times K}$. \n",
        "\n",
        "These bases in $B$ represent the set of common curves across time present in all players. By ensuring that $A$ and $C$ are identity matrices, \n",
        "the horizontal \"slices\" of the core tensor represent how much different metrics contributed to each basis **for each** slice (player). \n",
        "\n",
        "## Functional Basis Results \n"
      ],
      "id": "f32fc711"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with ui.layout_column_wrap():\n",
        "    @render_plotly\n",
        "    def plot_functional_bases_group():\n",
        "        dfs = []\n",
        "        for pos in positions:\n",
        "            factors = tensor_decomps[pos][\"factors\"]\n",
        "            funct_bases_df = pd.DataFrame(factors[1], columns = [f\"Basis {i}\" for i in range(1,4)])\n",
        "            funct_bases_df[\"Position\"] = pos\n",
        "            funct_bases_df[\"Age\"] = range(18,39)\n",
        "            dfs.append(funct_bases_df)\n",
        "        full_funct_bases = pd.concat(dfs)\n",
        "        funct_bases_melted = pd.melt(full_funct_bases, id_vars = [\"Position\", \"Age\"], value_vars = [f\"Basis {i}\" for i in range(1,4)],\n",
        "        var_name=\"basis_type\", value_name=\"value\")\n",
        "        fig = px.line(funct_bases_melted, x = \"Age\", y = \"value\", color=\"basis_type\", facet_col=\"Position\")\n",
        "        return fig\n",
        "    \n",
        "    @render_plotly\n",
        "    def plot_metric_weights_bases_group():\n",
        "        dfs = []\n",
        "        for pos in positions:\n",
        "            factors = tensor_decomps[pos][\"factors\"]\n",
        "            weights = tensor_decomps[pos][\"full_core\"].mean(0)\n",
        "            weights_df = pd.DataFrame(weights, columns = metrics)\n",
        "            weights_df[\"Position\"] = pos\n",
        "            weights_df[\"Basis\"] = range(1,4)\n",
        "            dfs.append(weights_df)\n",
        " \n",
        "        weights_df = pd.concat(dfs)\n",
        "        weights_df_melted = weights_df.melt(id_vars=[\"Basis\", \"Position\"], var_name=\"Variable\", value_name=\"Value\")\n",
        "        fig = px.bar(weights_df_melted, facet_row=\"Basis\",x=\"Variable\", y = \"Value\", facet_col=\"Position\")\n",
        "        return fig\n"
      ],
      "id": "5f99a576",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functional Basis Results (contd.)\n"
      ],
      "id": "ac3c46b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with ui.layout_column_wrap():\n",
        "    ui.input_select(id=\"player_functional\", label = \"Select a player\", choices = {index : name for index, name in enumerate(names)})\n",
        "with ui.layout_column_wrap():\n",
        "    @render_plotly\n",
        "    def plot_functional_bases():\n",
        "        player_index = int(input.player_functional())\n",
        "        full_core = tensor_decomps[\"global\"][\"full_core\"]\n",
        "        factors = tensor_decomps[\"global\"][\"factors\"]\n",
        "        explained_var = full_core[player_index].T.var(0)\n",
        "        funct_bases_df = pd.DataFrame(factors[1], columns = [f\"Basis {i}: {explained_var[i-1]/16:.{3}}% EV\" for i in range(1,4)])\n",
        "        funct_bases_df[\"Age\"] = range(18,39)\n",
        "        funct_bases_melted = funct_bases_df.melt(id_vars=\"Age\", var_name=\"Variable\", value_name=\"Value\")\n",
        "        fig = px.line(funct_bases_melted, x = \"Age\", y = \"Value\", color=\"Variable\")\n",
        "        return fig\n",
        "    @render_plotly\n",
        "    def plot_metric_weights():\n",
        "        player_index = int(input.player_functional())\n",
        "        full_core = tensor_decomps[\"global\"][\"full_core\"]\n",
        "        factors = tensor_decomps[\"global\"][\"factors\"]\n",
        "        weights = full_core[player_index]  \n",
        "        weights_df = pd.DataFrame(weights, columns = metrics)\n",
        "        weights_df[\"Basis\"] = range(1,4)\n",
        "        weights_df_melted = weights_df.melt(id_vars=\"Basis\", var_name=\"Variable\", value_name=\"Value\")\n",
        "        fig = px.bar(weights_df_melted, facet_row=\"Basis\",x=\"Variable\", y = \"Value\")\n",
        "        return fig"
      ],
      "id": "da7ad817",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Future Work \n",
        "\n",
        "\n",
        "1. Using a hybrid Gibbs-HMC routine, perform the following updates:\n",
        "    - Sample $X$, $\\gamma$ while holding all other parameters fixed using HMC proposal step\n",
        "    - Conditional on the sampled $X$ and $\\gamma$, sample the remaining parameters using HMC proposal step\n",
        "\n",
        "2. Posterior Coverage Tests\n",
        "    - Holdout forecasting performance\n",
        "\n",
        "# Questions ?\n",
        "\n",
        "# References\n",
        "\n",
        "::: {#refs}\n",
        ":::\n"
      ],
      "id": "d8a59bda"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}