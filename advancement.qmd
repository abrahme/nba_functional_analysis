---
title: "A Latent Variable Model for Modeling Multivariate Exponential Family Time-Series"
subtitle: With Applications to Sports Production Curve Modeling
author: Abhijit Brahme
format: 
    revealjs: 
        incremental: false
        smaller: true
        scrollable: true
        html-math-method: mathjax
server: shiny
date: 2024-09-20
bibliography: advancement.bib
---




```{python}
#| context: setup
#| include: false
import pandas as pd
import numpy as np
from shiny.express import input, render, ui
from shinywidgets import render_plotly
```

```{python}
#| context: setup
#| include: false
#| cache: true
from data.data_utils import create_fda_data
data = pd.read_csv("data/player_data.csv").query(" age <= 38 ")
names = data.groupby("id")["name"].first().values
names_df = pd.DataFrame(names, columns = ["Name"])
names_df["Player"] = range(len(names))
metric_output = ["binomial", "exponential"] + (["gaussian"] * 2) + (["poisson"] * 9) + (["binomial"] * 3)
metrics = ["retirement", "minutes", "obpm","dbpm","blk","stl","ast","dreb","oreb","tov","fta","fg2a","fg3a","ftm","fg2m","fg3m"]
metric_df = pd.DataFrame(metrics, columns=["Statistic"])
metric_df["Metric"] = range(len(metrics))
age_df = pd.DataFrame(range(18,39), columns = ["Age"])
age_df["Time"] = age_df["Age"] - 18
exposure_list = (["simple_exposure"] * 2) + (["minutes"] * 11) + ["fta","fg2a","fg3a"]
data["retirement"] = 1
data["log_min"] = np.log(data["minutes"])
data["simple_exposure"] = 1 
_ , outputs, _ = create_fda_data(data, basis_dims=3, metric_output=metric_output, 
                                     metrics = metrics
, exposure_list =  exposure_list)
observations = np.stack([output["output_data"] for output in outputs], axis = 1)
exposures = np.stack([output["exposure_data"] for output in outputs], axis = 0)
```


```{python}
#| context: setup
#| include: false
#| cache: true

N, K, T = observations.shape

edges = []
nodes = []
vals = []
missing = []
for player_index in range(N):
    for metric_index in range(K):
        for time_index in range(T):
            cur_node = (player_index, metric_index, time_index)
            nodes.append(cur_node)
            missing.append(np.isfinite(exposures[metric_index, player_index, time_index]))
            vals.append(observations[player_index, metric_index, time_index])

            if player_index != N - 1:
                edges.append((cur_node, (player_index + 1, metric_index ,time_index)))

            if player_index != 0:
                edges.append((cur_node, (player_index - 1, metric_index, time_index )))
            
            if metric_index != K - 1:
                edges.append((cur_node, (player_index, metric_index + 1, time_index)))

            if metric_index != 0:
                edges.append((cur_node, (player_index, metric_index - 1, time_index)))

            if time_index != T - 1:
                edges.append((cur_node, (player_index, metric_index, time_index + 1)))

            if time_index != 0:
                edges.append((cur_node, (player_index, metric_index, time_index - 1)))

nodes_df = pd.DataFrame(np.array([list(x) for x in zip(*nodes)]).T, columns=["Player", "Metric", "Time"])
nodes_df["Value"] = vals
nodes_df["Missing"] = missing

final_data_plot_df = pd.merge(nodes_df, names_df).merge(metric_df).merge(age_df)

```

## Research Interests 

::: {layout-ncol=2}
![](images/noun-deep-learning-1705425.png)

![](images/noun-scatter-graph-4768711.png)
:::

## Current Work

1. Metric / Manifold Estimation from Trajectory Valued Data 
    - Prof. Nina Mialone (Dept. of Electrical Engineering)
2. Approximate HMC Methods
    - Prof. Andrew Holbrook (UCLA, Dept of Public Health)
3. **A Latent Variable Model for Modeling Multi-Dimensional Exponential Family Time-Series**
    - **Prof. Alex Franks (Dept. of Statistics)**

## Motivation 

```{python}

from visualization.visualization import plot_career_trajectory_observations
ui.input_select(id="player", label = "Select a player", choices = {index : name for index, name in enumerate(names)})
@render_plotly
def plot_metric_arc():
    return plot_career_trajectory_observations(int(input.player()), metrics, metric_output, observations, exposures )

```

## Goals

1. Successfully forecast player performance, especially young players
2. Gain insight into how different observable metrics may peak at varying timepoints for certain groups of players
3. Quantify a level of uncertainty regarding players' future performance


## Relevant Work

1. Bayesian Hierarchical Framework
    - Hierarchical aging model applied to hockey, golf, and baseball [@berry1999bridging]
    - Gaussian Process regressions by different basketball positions [@page2013effect]
    - Parametric curves analyzing pre and post peak-performance [@vaci2019large]

2. Functional Data Analysis
    - Functional principal components clustering [@wakim2014functional]
    - Nearest Neighbor algorithm to characterize similarity between players [@natesilver538_2015]
    - Production curves as a convex combination of curves from the same set of archetype [@vinue2015archetypoids]


## Data Overview
1. $\approx$ 2k NBA players from years 1997 - 2021, from the ages of 18 - 39
2. Longitudinal mixed tensor valued data $\mathcal{Y}$ of size $N$ by $T$ by $K$
where $N$ is the number of players, $T$ is the number of years in a player's career, and $K$ are the number of production metric curves with $\mathcal{Y}_{ntk}$ is missing if player $n$ is not observed for metric $k$ at age $t$. 
    - Non-missing entries are observations from exponential families (i.e Binomial, Gaussian, Exponential, Poisson, etc.)
3. $\Omega$ is binary tensor of same size as $\mathcal{Y}$ indicating missingness. 


```{python}
from visualization.visualization import plot_data_tensor
@render_plotly
def plot_data():
    return plot_data_tensor(final_data_plot_df)
```


## Modeling Setup
Space of players live in low dimensional latent space $X \in \mathbb{R} ^{N \times D}$

For a given time $t$, and metric $k$, $f^k_t \sim \mathcal{GP}(0, K_X)$ is a vector of size $N$, with $K_X$ capturing correlation across the $N$ dimension

![](images/pgm.svg)

<!-- 2. For a given time $t$, and metric $k$, $f^t_k \sim \mathcal{GP}(0, K_X)$ is a vector of size $N$, with $K_X$ capturing correlation between players
    -  -->


## Random Fourier Features (tl;dr)

1. Approximation of Gaussian Process can be turned into a linear operation, $f^k_{t}(X) \approx Z(X)^T\beta^k_{t}$ which is computationally beneficial if $N$ large
2. Number of random features, $R$, determines how good the approximation is

## Random Fourier Features 

Attempt to approximate the inner product $k(x, y) = \langle \phi(x), \phi(y) \rangle$
with a randomized map $z: \mathbb{R}^D \rightarrow \mathbb{R}^R$. Computational savings arise if $R << N$.

In our case, we let $k(x,y) = k(x - y) = exp(\frac{-||x - y||^2}{2})$ be the standard radial basis kernel. 

From Bochner's theorem, we have that 
$k(x - y) = \int p(\omega)exp(i\omega(x - y))d \omega$, and it can be shown that to produce the radial basis kernel, $\omega \sim \mathcal{N}_D(0, I_d)$. 

Thus the map is composed of $z_{\omega_r} = [cos(\omega_r ^T x), sin(\omega_r ^ T x)]^T$. 

$Z(X) = \frac{1}{\sqrt{R}}[z_{\omega_1}, z_{\omega_2}, \dots, z_{\omega_R}]^T$

## Adjusted Modeling Setup

We will approximate each $f^k_t$ using the aforementioned RFF approximation. Temporal correlation is induced through a covariance structure across time, enforced on the linear weights of the approximation.

![](images/pmg_corr.svg)





## Distributional Assumptions

We include the following metrics and distribution families

1. Poisson
    - $\mathcal{R} =$ {FG2A, FG3A, FTA, BLK, OREB, DREB, TOV, AST, STL}
2. Gaussian
    - $\mathcal{G} =$ {DBPM, OBPM}
3. Binomial
    - $\mathcal{B} =$ {FG2M, FG3M, FTM}
     - $\mathcal{A} =$ {FG2A, FG3A, FTA}
4. Exponential
    - $\mathcal{M} =$ {Minutes}
5. Bernoulli
    - $\mathcal{K} =$ {Retirement}

\begin{align}
Y^k_{t}  &\sim
\begin{cases}
Pois(f^k_{t}e^{Y^m_t}) \text{if } k \in \mathcal{R} \text{ , } \forall m \in \mathcal{M} \\
Bin(Y^j_{t}, logit^{-1}(f^k_{t})) \text{ if } k \in \mathcal{B} \text{ , }  j \in \mathcal{A} \\
\mathcal{N}(f^k_{t}, \frac{\sigma^2_k}{Y^m_t})  \text{ if } k \in \mathcal{G} \text{ , } \forall m \in \mathcal{M}\\
Bern(logit^{-1}(f^k_t)) \text{ if } k \in \mathcal{K}\\
Exp(e^{f^k_t}) \text{ if  } k \in \mathcal{M}
\end{cases}
\end{align}



## Inferential Challenges 

1. MCMC Convergence (multi-modal posterior)
2. Identifiability (rotational / scale invariance of model)
3. Modeling temporal and within-metric correlation

## Methods 

In order to address identifiability issues and MCMC convergence, we propose the following scheme to estimate the latent space $X$ and functional coefficients $\beta_{rtk}$. 

1. Initialize $X$ 
    - Exponential PPCA, Probabilistic Tensor Decomposition, Standard PCA, etc.
2. Using the fixed $X$ from above, conduct inference on $\beta_{rtk}, \sigma_k, \omega_r$ 

## Probabilistic Tensor Decomposition 

This model seeks to factorize the  $N \times T \times K$ linear scale tensor $A$ using CP Decomposition. Since we have various outputs that are not normally distributed, this becomes a form of exponential family CP Decomposition.

We seek to approximate the following:

$A \approx \mu + \sum_{i=1}^{R} \lambda_i \cdot x_i \otimes v_i \otimes w_i$

where, $\mu$, $x_i$, $v_i$, $w_i$ $\sim \mathcal{N}(0, I)$, $\lambda \sim Dirichlet(1/R)$

$X \in \mathbb{R}^{N \times R}$

$V \in \mathbb{R}^{T \times R}$

$W \in \mathbb{R}^{K \times R}$

$\mu \in \mathbb{R}^{T \times K}$. 

Here $\mu$ is used to de-mean the data and act as an intercept term.  

## Probabilistic Tensor Decomposition (contd.) 

Let $\tilde{A}_{pit} = g_{pit}^{-1}(A_{pit})$, where $g_{pit}$ is the appropriate link function transforming the linear scale parameter into the appropriate exponential family parameterization. Consequently, $X, V, W, \mu$ are estimated by maximizing the following loss function using gradient descent. 

$\max_{X, V, W, \mu} \sum_{p, i, t} log( F_{pit}( Y_{pit} | \tilde{A}_{pit} )) \cdot \Omega_{pit}$

where $F_{pit}$ is the appropriate distribution density function associated with entry $Y_{pit}$. 

This offers the following benefits:

1. Latent space $X$ is created while accounting for sampling variability 
2. Latent space is created while also accounting for correlations across each mode of the tensor, which is representative of the final model. 


## Latent Space 


## Sampling Results 


## Projection Results 


## Functional Basis Results 


## Future Work 

In order to address identifiability issues and MCMC convergence while also recovering sampling variability in the latent space, we propose an alternating scheme to estimate the latent space $X$ and functional coefficients $\beta_{rtk}$. 

1. Let $X \sim \mathcal{N}(\mu_0, \Sigma_0)$ where $\mu_0$ and $\Sigma_0$ come from an initialized latent space $X_0$.
    - Exponential PPCA, Probabilistic Tensor Decomposition, Standard PCA, etc. can be used to create $X_0$
2. Using a hybrid Gibbs-HMC routine, perform the following updates:
    - Sample $X$, $\gamma$ while holding all other parameters fixed using HMC proposal step
    - Conditional on the sampled $X$ and $\gamma$, sample the remaining parameters using HMC proposal step

## Future Work (contd.)
In order to address identifiability issues and MCMC convergence while also recovering sampling variability in the latent space, we propose an alternating scheme to estimate the latent space $X$ and functional coefficients $\beta_{rtk}$. 

1. Let $X \sim \mathcal{N}(\mu_0, \Sigma_0)$ where $\mu_0$ and $\Sigma_0$ come from an initialzed latent space $X_0$.
    - Exponential PPCA, Probabilistic Tensor Decomposition, Standard PCA, etc. can be used to create $X_0$
2. Conditional on the fixed $X_0$, sample the remaining parameters using HMC until convergence.
3. Taking the posterior mean of all parameters resulting from (2), sample $X$ using HMC until convergence. 


# References

::: {#refs}
:::


# Appendix {.appendix}



